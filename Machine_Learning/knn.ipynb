{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAfFPP1_4TYQ",
        "outputId": "463f8012-dcd8-4756-f3a7-aa27a0ab542b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing complete.\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Define Gabor filter parameters\n",
        "orientations = 8\n",
        "frequencies = [0.1, 0.5, 1.0]\n",
        "kernel_size = 21  # Adjust this based on your image size and requirements\n",
        "\n",
        "# Specify the folder containing your images\n",
        "image_folder = '/content/sample_data/dataset'\n",
        "\n",
        "# Create an output folder to save the preprocessed images\n",
        "output_folder = '/content/sample_data/preprocessed'\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Initialize the Gabor filter bank\n",
        "gabor_filters = []\n",
        "\n",
        "# Create the Gabor filters\n",
        "for theta in range(orientations):\n",
        "    for freq in frequencies:\n",
        "        kernel = cv2.getGaborKernel(\n",
        "            (kernel_size, kernel_size),\n",
        "            sigma=4.0,  # Adjust the sigma value as needed\n",
        "            theta=theta * (np.pi / orientations),\n",
        "            lambd=10.0 / freq,\n",
        "            gamma=0.5,\n",
        "            psi=0,\n",
        "        )\n",
        "        gabor_filters.append(kernel)\n",
        "\n",
        "# Process each image in the folder\n",
        "for filename in os.listdir(image_folder):\n",
        "    if filename.endswith(('.jpg', '.jpeg', '.png')):  # Filter for image file extensions\n",
        "        # Load the image\n",
        "        image_path = os.path.join(image_folder, filename)\n",
        "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        # Initialize an empty list to store the filtered images\n",
        "        filtered_images = []\n",
        "\n",
        "        # Apply the Gabor filters to the image\n",
        "        for kernel in gabor_filters:\n",
        "            filtered_image = cv2.filter2D(image, cv2.CV_64F, kernel)\n",
        "            filtered_images.append(filtered_image)\n",
        "\n",
        "        # Save the preprocessed images to the output folder\n",
        "        output_filename = os.path.splitext(filename)[0] + '_preprocessed.jpg'\n",
        "        output_path = os.path.join(output_folder, output_filename)\n",
        "        preprocessed_image = np.hstack(filtered_images)  # Combine filtered images side by side\n",
        "        cv2.imwrite(output_path, preprocessed_image)\n",
        "\n",
        "print(\"Preprocessing complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile"
      ],
      "metadata": {
        "id": "W9aL6z4KHFI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with ZipFile('/content/MICC-F220.zip','r') as z:\n",
        "  z.extractall(\"/content/sample_data/dataset\")"
      ],
      "metadata": {
        "id": "v570F-gfISo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install zipfile36"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dl14gO57HnZK",
        "outputId": "4c87e021-9524-4da3-8107-3514ba5ddafb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting zipfile36\n",
            "  Downloading zipfile36-0.1.3-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: zipfile36\n",
            "Successfully installed zipfile36-0.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from skimage import feature\n",
        "\n",
        "# Specify the folder containing your preprocessed images\n",
        "preprocessed_folder = '/content/sample_data/preprocessed'\n",
        "\n",
        "# Create an output folder to save the LBP feature vectors\n",
        "lbp_output_folder = '/content/sample_data/features'\n",
        "os.makedirs(lbp_output_folder, exist_ok=True)\n",
        "\n",
        "# LBP parameters\n",
        "radius = 1\n",
        "n_points = 8 * radius\n",
        "\n",
        "# Process each preprocessed image\n",
        "for filename in os.listdir(preprocessed_folder):\n",
        "    if filename.endswith(('_preprocessed.jpg')):  # Filter for preprocessed images\n",
        "        # Load the preprocessed image\n",
        "        image_path = os.path.join(preprocessed_folder, filename)\n",
        "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        # Compute LBP features\n",
        "        lbp_image = feature.local_binary_pattern(image, n_points, radius, method='uniform')\n",
        "\n",
        "        # Flatten the LBP image into a feature vector\n",
        "        lbp_feature_vector, _ = np.histogram(lbp_image.ravel(), bins=np.arange(0, n_points + 3), range=(0, n_points + 2))\n",
        "\n",
        "        # Save the LBP feature vector as a CSV file or any other format you prefer\n",
        "        output_filename = os.path.splitext(filename)[0] + '_lbp.csv'\n",
        "        output_path = os.path.join(lbp_output_folder, output_filename)\n",
        "        np.savetxt(output_path, lbp_feature_vector, delimiter=',')\n",
        "\n",
        "print(\"LBP feature extraction complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xvb1quo9DXf",
        "outputId": "bbbc310e-0220-43ec-b09d-3638d195cfa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LBP feature extraction complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "\n",
        "# Load LBP features and labels\n",
        "lbp_feature_folder = '/content/sample_data/features'\n",
        "labels = []  # Store class labels\n",
        "feature_vectors = []  # Store LBP feature vectors\n",
        "\n",
        "# Iterate through the LBP feature files\n",
        "for filename in os.listdir(lbp_feature_folder):\n",
        "    if filename.endswith('_lbp.csv'):\n",
        "        # Extract class label from the filename\n",
        "        class_label = filename.split('_')[0]\n",
        "\n",
        "        # Load LBP feature vector\n",
        "        feature_vector = np.loadtxt(os.path.join(lbp_feature_folder, filename), delimiter=',')\n",
        "\n",
        "        labels.append(class_label)\n",
        "        feature_vectors.append(feature_vector)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(feature_vectors, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check if there are classes with very few samples\n",
        "class_counts = Counter(y_train)\n",
        "min_class_count = min(class_counts.values())\n",
        "\n",
        "# Apply SMOTE only if the minimum class count is greater than 1\n",
        "if min_class_count > 1:\n",
        "    # Apply SMOTE to handle imbalanced classes\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "    # Feature scaling\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "else:\n",
        "    # Use the original data without SMOTE\n",
        "    X_train_scaled = StandardScaler().fit_transform(X_train)\n",
        "    X_test_scaled = StandardScaler().fit_transform(X_test)\n",
        "    y_train_resampled = y_train\n",
        "\n",
        "# Initialize and train a K-Nearest Neighbors (KNN) classifier\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors (K) as needed\n",
        "knn_classifier.fit(X_train_scaled, y_train_resampled)\n",
        "\n",
        "# Predict the labels for the scaled test data\n",
        "y_pred = knn_classifier.predict(X_test_scaled)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print classification report for more detailed metrics\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50_o7HuzfyJ4",
        "outputId": "7c369a89-8b9c-450c-bf5b-c49e5fb1d771"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 47.73%\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          CRW       0.57      1.00      0.73         8\n",
            "          DSC       0.90      0.60      0.72        15\n",
            "       DSCF10       0.00      0.00      0.00         0\n",
            "       DSCF13       0.00      0.00      0.00         1\n",
            "       DSCF15       0.00      0.00      0.00         0\n",
            "        DSCF2       0.00      0.00      0.00         1\n",
            "     DSCF2059       0.00      0.00      0.00         0\n",
            "   DSCF8tamp1       0.00      0.00      0.00         0\n",
            " DSCF8tamp132       0.00      0.00      0.00         1\n",
            " DSCF8tamp133       0.00      0.00      0.00         0\n",
            " DSCF8tamp237       0.00      0.00      0.00         1\n",
            "  DSCF8tamp25       0.00      0.00      0.00         1\n",
            "        DSCF9       0.00      0.00      0.00         1\n",
            "     DSCN2322       0.00      0.00      0.00         0\n",
            "     DSCN2329       0.00      0.00      0.00         1\n",
            "       DSCN41       0.00      0.00      0.00         0\n",
            "  DSCN41tamp1       0.00      0.00      0.00         0\n",
            "DSCN41tamp131       0.00      0.00      0.00         1\n",
            "DSCN41tamp132       0.00      0.00      0.00         1\n",
            "       DSCN45       0.00      0.00      0.00         1\n",
            "  DSCN45tamp1       0.00      0.00      0.00         1\n",
            "DSCN45tamp131       0.00      0.00      0.00         0\n",
            "DSCN45tamp132       0.00      0.00      0.00         1\n",
            "DSCN45tamp237       0.00      0.00      0.00         1\n",
            "     P1000528       0.00      0.00      0.00         1\n",
            "         sony       1.00      0.57      0.73         7\n",
            "\n",
            "     accuracy                           0.48        44\n",
            "    macro avg       0.10      0.08      0.08        44\n",
            " weighted avg       0.57      0.48      0.49        44\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load LBP features and labels\n",
        "lbp_feature_folder = '/content/sample_data/features'\n",
        "labels = []  # Store class labels\n",
        "feature_vectors = []  # Store LBP feature vectors\n",
        "\n",
        "# Iterate through the LBP feature files\n",
        "for filename in os.listdir(lbp_feature_folder):\n",
        "    if filename.endswith('_lbp.csv'):\n",
        "        # Extract class label from the filename\n",
        "        class_label = filename.split('_')[0]\n",
        "\n",
        "        # Load LBP feature vector\n",
        "        feature_vector = np.loadtxt(os.path.join(lbp_feature_folder, filename), delimiter=',')\n",
        "\n",
        "        labels.append(class_label)\n",
        "        feature_vectors.append(feature_vector)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(feature_vectors, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the KNN classifier\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors (K) as needed\n",
        "\n",
        "# Train the classifier on the training data\n",
        "knn_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test data\n",
        "y_pred = knn_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFDdUBTwGj6I",
        "outputId": "98649ebc-0508-405f-b56e-11ec96ec5735"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 52.27%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load LBP features and labels\n",
        "lbp_feature_folder = '/content/sample_data/features'\n",
        "labels = []  # Store class labels\n",
        "feature_vectors = []  # Store LBP feature vectors\n",
        "\n",
        "# Iterate through the LBP feature files\n",
        "for filename in os.listdir(lbp_feature_folder):\n",
        "    if filename.endswith('_lbp.csv'):\n",
        "        # Extract class label from the filename\n",
        "        class_label = filename.split('_')[0]\n",
        "\n",
        "        # Load LBP feature vector\n",
        "        feature_vector = np.loadtxt(os.path.join(lbp_feature_folder, filename), delimiter=',')\n",
        "\n",
        "        labels.append(class_label)\n",
        "        feature_vectors.append(feature_vector)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(feature_vectors, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Hyperparameter tuning using GridSearchCV\n",
        "param_grid = {'n_neighbors': [1, 3, 5, 7, 9]}\n",
        "knn_classifier = KNeighborsClassifier()\n",
        "grid_search = GridSearchCV(knn_classifier, param_grid, cv=5)\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "best_k = grid_search.best_params_['n_neighbors']\n",
        "\n",
        "# Initialize the KNN classifier with the best K\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=best_k)\n",
        "\n",
        "# Train the classifier on the scaled training data\n",
        "knn_classifier.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict the labels for the scaled test data\n",
        "y_pred = knn_classifier.predict(X_test_scaled)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaj1c34Fdu3Z",
        "outputId": "376f5d22-68ce-41ba-cda2-4200b41c8af5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 47.73%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "\n",
        "# Load LBP features and labels\n",
        "lbp_feature_folder = '/content/sample_data/features'\n",
        "labels = []  # Store class labels\n",
        "feature_vectors = []  # Store LBP feature vectors\n",
        "\n",
        "# Iterate through the LBP feature files\n",
        "for filename in os.listdir(lbp_feature_folder):\n",
        "    if filename.endswith('_lbp.csv'):\n",
        "        # Extract class label from the filename\n",
        "        class_label = filename.split('_')[0]\n",
        "\n",
        "        # Load LBP feature vector\n",
        "        feature_vector = np.loadtxt(os.path.join(lbp_feature_folder, filename), delimiter=',')\n",
        "\n",
        "        labels.append(class_label)\n",
        "        feature_vectors.append(feature_vector)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(feature_vectors, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check if there are classes with very few samples\n",
        "class_counts = Counter(y_train)\n",
        "min_class_count = min(class_counts.values())\n",
        "\n",
        "# Apply SMOTE only if the minimum class count is greater than 1\n",
        "if min_class_count > 1:\n",
        "    # Apply SMOTE to handle imbalanced classes\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "    # Feature scaling\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "else:\n",
        "    # Use the original data without SMOTE\n",
        "    X_train_scaled = StandardScaler().fit_transform(X_train)\n",
        "    X_test_scaled = StandardScaler().fit_transform(X_test)\n",
        "    y_train_resampled = y_train\n",
        "\n",
        "# Initialize and train a Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train_scaled, y_train_resampled)\n",
        "\n",
        "# Predict the labels for the scaled test data\n",
        "y_pred = rf_classifier.predict(X_test_scaled)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print classification report for more detailed metrics\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qY-Sz1Z3fHDj",
        "outputId": "c4dc3fdf-0ee4-44af-cf53-98c26d1aa07b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 56.82%\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          CRW       0.80      1.00      0.89         8\n",
            "          DSC       0.93      0.87      0.90        15\n",
            "       DSCF10       0.00      0.00      0.00         0\n",
            "       DSCF13       0.00      0.00      0.00         1\n",
            "        DSCF2       0.00      0.00      0.00         1\n",
            "        DSCF6       0.00      0.00      0.00         0\n",
            "   DSCF8tamp1       0.00      0.00      0.00         0\n",
            " DSCF8tamp132       0.00      0.00      0.00         1\n",
            " DSCF8tamp237       0.00      0.00      0.00         1\n",
            "  DSCF8tamp25       0.00      0.00      0.00         1\n",
            "  DSCF8tamp27       0.00      0.00      0.00         0\n",
            "  DSCF8tamp37       0.00      0.00      0.00         0\n",
            "        DSCF9       0.00      0.00      0.00         1\n",
            "     DSCN2329       0.00      0.00      0.00         1\n",
            "       DSCN41       0.00      0.00      0.00         0\n",
            "DSCN41tamp131       0.00      0.00      0.00         1\n",
            "DSCN41tamp132       0.00      0.00      0.00         1\n",
            "       DSCN43       0.00      0.00      0.00         0\n",
            "       DSCN45       0.00      0.00      0.00         1\n",
            "  DSCN45tamp1       0.00      0.00      0.00         1\n",
            "DSCN45tamp132       0.00      0.00      0.00         1\n",
            "DSCN45tamp134       0.00      0.00      0.00         0\n",
            "DSCN45tamp237       0.00      0.00      0.00         1\n",
            " DSCN45tamp27       0.00      0.00      0.00         0\n",
            "       DSCN48       0.00      0.00      0.00         0\n",
            "     P1000528       0.00      0.00      0.00         1\n",
            "         sony       0.67      0.57      0.62         7\n",
            "\n",
            "     accuracy                           0.57        44\n",
            "    macro avg       0.09      0.09      0.09        44\n",
            " weighted avg       0.57      0.57      0.57        44\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from skimage.feature import local_binary_pattern\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Define parameters\n",
        "patch_size = (32, 32)  # Adjust patch size as needed\n",
        "num_neighbors = 5  # Number of nearest neighbors in KNN\n",
        "threshold = 0.5  # Adjust the similarity threshold\n",
        "\n",
        "# Function to extract features from an image patch (replace with your feature extraction method)\n",
        "def extract_features(patch):\n",
        "    # Example: Using Local Binary Pattern (LBP) as a feature\n",
        "    lbp_image = local_binary_pattern(patch, P=8, R=1, method='uniform')\n",
        "    histogram, _ = np.histogram(lbp_image, bins=np.arange(0, 60), range=(0, 59))\n",
        "    return histogram\n",
        "\n",
        "# Function to load training data (replace with your own dataset loading)\n",
        "def load_training_data():\n",
        "    # Placeholder data (replace with your dataset loading code)\n",
        "    num_samples = 200\n",
        "    num_features = 59\n",
        "    feature_vectors_train = np.random.rand(num_samples, num_features)\n",
        "    labels_train = np.random.randint(2, size=num_samples)\n",
        "    return feature_vectors_train, labels_train\n",
        "\n",
        "# Function to detect forgeries in an image\n",
        "def detect_forgeries(input_image_path, knn_classifier, scaler, feature_vectors_train):\n",
        "    input_image = cv2.imread(input_image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    height, width = input_image.shape\n",
        "    forgery_detection_image = input_image.copy()\n",
        "\n",
        "    for y in range(0, height - patch_size[1] + 1):\n",
        "        for x in range(0, width - patch_size[0] + 1):\n",
        "            patch = input_image[y:y + patch_size[1], x:x + patch_size[0]]\n",
        "            features = extract_features(patch)\n",
        "            standardized_features = scaler.transform([features])\n",
        "\n",
        "            _, indices = knn_classifier.kneighbors(standardized_features)\n",
        "\n",
        "            for index in indices[0]:\n",
        "                similarity = np.sum((feature_vectors_train[index] - standardized_features) ** 2)\n",
        "                if similarity < threshold:\n",
        "                    forgery_detection_image[y:y + patch_size[1], x:x + patch_size[0]] = 255  # White patch\n",
        "\n",
        "    output_image_path = 'output_' + os.path.basename(input_image_path)\n",
        "    cv2.imwrite(output_image_path, forgery_detection_image)\n",
        "\n",
        "# Load the training dataset (authentic and tampered images)\n",
        "feature_vectors_train, labels_train = load_training_data()\n",
        "\n",
        "# Standardize the feature vectors\n",
        "scaler = StandardScaler()\n",
        "feature_vectors_train = scaler.fit_transform(feature_vectors_train)\n",
        "\n",
        "# Initialize and train the KNN model\n",
        "knn_classifier = NearestNeighbors(n_neighbors=num_neighbors)\n",
        "knn_classifier.fit(feature_vectors_train)\n",
        "\n",
        "# Specify the folder containing the images to be processed\n",
        "input_image_folder = '/content/sample_data/dataset'\n",
        "\n",
        "# Process each image in the folder\n",
        "for filename in os.listdir(input_image_folder):\n",
        "    if filename.endswith(('.jpg', '.jpeg', '.png')):  # Filter for image file extensions\n",
        "        input_image_path = os.path.join(input_image_folder, filename)\n",
        "        detect_forgeries(input_image_path, knn_classifier, scaler, feature_vectors_train)\n"
      ],
      "metadata": {
        "id": "XAAbi7vagtK7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}